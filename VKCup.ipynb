{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных"
      ],
      "metadata": {
        "id": "cuEEDcwBxsZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QA_Ndm4ywBI9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "from itertools import chain\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import transformers\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.normalizers import Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtype, device, cuda_device_id = torch.float32, None, 0\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '{0}'.format(str(cuda_device_id) if cuda_device_id is not None else '')\n",
        "if cuda_device_id is not None and torch.cuda.is_available():\n",
        "    device = 'cuda:{0:d}'.format(0)\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(f'Using device: {device}, dtype: {dtype}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpmqXPRlwEoZ",
        "outputId": "1df11213-1134-486a-efef-f64f105f2d17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu, dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "embedding_dim_ua = 1024\n",
        "hidden_dim_ua = 1024\n",
        "num_epochs = 15\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "vocab_size = 1000\n",
        "nhead = 4\n",
        "d_hid = 1024\n",
        "nlayers = 4\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "gHVAvAATwGcO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_parquet('train.parquet')\n",
        "test_dataset = pd.read_parquet('test.parquet')"
      ],
      "metadata": {
        "id": "iJV0yWsbwR5D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(inputs):\n",
        "    return ast.literal_eval(inputs.decode(\"utf-8\"))\n",
        "\n",
        "test_dataset.curves = test_dataset.curves.apply(f1)\n",
        "test_dataset.ciphers = test_dataset.ciphers.apply(f1)"
      ],
      "metadata": {
        "id": "Tfh6XOodwSDy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "        self.rows = []\n",
        "        for i, row in data.iterrows():\n",
        "            self.rows.append(row)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.rows[idx]\n",
        "        ciphers = torch.tensor(ciphers_vocab.lookup_indices(['<sos>']) + ciphers_vocab.lookup_indices(list(row.ciphers)) + ciphers_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        curves = torch.tensor(curves_vocab.lookup_indices(['<sos>']) + curves_vocab.lookup_indices(list(row.curves)) + curves_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        return ciphers, curves, torch.tensor(len(row.ciphers) + 2), torch.tensor(len(row.curves) + 2), torch.tensor(row.label, dtype=torch.float)\n",
        "    \n",
        "class TestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "        self.rows = []\n",
        "        for i, row in data.iterrows():\n",
        "            self.rows.append(row)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.rows[idx]\n",
        "        ciphers = torch.tensor(ciphers_vocab.lookup_indices(['<sos>']) + ciphers_vocab.lookup_indices(list(row.ciphers)) + ciphers_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        curves = torch.tensor(curves_vocab.lookup_indices(['<sos>']) + curves_vocab.lookup_indices(list(row.curves)) + curves_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        return ciphers, curves, torch.tensor(len(row.ciphers) + 2), torch.tensor(len(row.curves) + 2)\n",
        "\n",
        "def collate_to_train_batch(batch):\n",
        "    ciphers = torch.nn.utils.rnn.pad_sequence([i[0] for i in batch], padding_value=0)\n",
        "    curves = torch.nn.utils.rnn.pad_sequence([i[1] for i in batch], padding_value=0)\n",
        "    return ciphers, curves, torch.tensor([i[2] for i in batch], dtype=torch.long), torch.tensor([i[3] for i in batch], dtype=torch.long), torch.tensor([i[4] for i in batch], dtype=torch.float)\n",
        "\n",
        "def collate_to_test_batch(batch):\n",
        "    ciphers = torch.nn.utils.rnn.pad_sequence([i[0] for i in batch], padding_value=0)\n",
        "    curves = torch.nn.utils.rnn.pad_sequence([i[1] for i in batch], padding_value=0)\n",
        "    return ciphers, curves, torch.tensor([i[2] for i in batch], dtype=torch.long), torch.tensor([i[3] for i in batch], dtype=torch.long)"
      ],
      "metadata": {
        "id": "MNS6KOWWwSGX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_dropout_mask(num_objects, input_size, hidden_size, is_training, p, some_existing_tensor):\n",
        "    if p is None:\n",
        "        return some_existing_tensor.new_ones((num_objects, input_size)), some_existing_tensor.new_ones((num_objects, hidden_size))\n",
        "    if is_training:\n",
        "        return torch.bernoulli((1 - p) * some_existing_tensor.new_ones((num_objects, input_size))), torch.bernoulli((1 - p) * some_existing_tensor.new_ones((num_objects, hidden_size)))\n",
        "    return (1 - p) * some_existing_tensor.new_ones((num_objects, input_size)), (1 - p) * some_existing_tensor.new_ones((num_objects, hidden_size))\n",
        "\n",
        "class FastRNNLayer(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.0, layers_dropout=0.0, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.layers_dropout = layers_dropout\n",
        "        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout, num_layers=num_layers)\n",
        "        self.layer_names = []\n",
        "        for layer_n in range(self.num_layers):\n",
        "            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n",
        "        for layer in self.layer_names:\n",
        "            w = getattr(self.module, layer)\n",
        "            delattr(self.module, layer)\n",
        "            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n",
        "\n",
        "    def _setweights(self, x):\n",
        "        for layer in self.layer_names:\n",
        "            raw_w = getattr(self, f'{layer}_raw')\n",
        "            mask, _ = gen_dropout_mask(1, raw_w.shape[1], self.hidden_size, self.training, self.dropout, x)\n",
        "            masked_raw_w = raw_w * mask\n",
        "            setattr(self.module, layer, masked_raw_w)\n",
        "\n",
        "    def forward(self, x, h_c=None):\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            self._setweights(x)\n",
        "            if h_c is not None:\n",
        "                return self.module.forward(x, h_c)\n",
        "            return self.module.forward(x)\n",
        "            \n",
        "    def reset(self):\n",
        "        if hasattr(self.module, 'reset'):\n",
        "            self.module.reset()"
      ],
      "metadata": {
        "id": "qFP1TEuTwSI1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение TLS-эмбеддингов"
      ],
      "metadata": {
        "id": "VPa77zIPwy29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для построения TLS-эмбеддинга будем использовать автокодировщик. Его идея такая: на вход подается шифр/кривая объекта, по ним проходится LSTM, а потом эта же LSTM должна воспроизвести то, что прочитала. Мы заводим два автокодировщика на шифры и кривые: один читает слева направо, другой справа налево. Автокодировщики обучаются на всех данных: на трейне, тесте и неразмеченном датасете. Код ниже работает достаточно долго, сохраненные веса автокодировщиков есть в архиве"
      ],
      "metadata": {
        "id": "p7g19WXwKI1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unlab = pd.read_parquet('unlabelled.snappy.parquet')\n",
        "big_dataset = pd.concat([train_dataset.drop(['id', 'label'], axis=1), test_dataset.drop(['id'], axis=1), unlab])"
      ],
      "metadata": {
        "id": "mSUrSRTBwSOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ciphers_big_dataset = Counter()\n",
        "for i, row in big_dataset.iterrows():\n",
        "    ciphers_big_dataset = ciphers_big_dataset + Counter(list(map(str, row.ciphers)))\n",
        "\n",
        "curves_big_dataset = Counter()\n",
        "for i, row in big_dataset.iterrows():\n",
        "    curves_big_dataset = curves_big_dataset + Counter(list(map(str, row.curves)))\n",
        "    \n",
        "specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "for special in specials:\n",
        "    ciphers_big_dataset[special] = 0\n",
        "    curves_big_dataset[special] = 0\n",
        "ciphers_vocab = torchtext.vocab.vocab(dict(sorted(ciphers_big_dataset.items(), key=lambda x: x[1], reverse=True)), specials=specials)\n",
        "ciphers_vocab.set_default_index(ciphers_vocab['<unk>'])\n",
        "curves_vocab = torchtext.vocab.vocab(dict(sorted(curves_big_dataset.items(), key=lambda x: x[1], reverse=True)), specials=specials)\n",
        "curves_vocab.set_default_index(curves_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "I1su500EwSQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    TestDataset(big_dataset), batch_size=batch_size, collate_fn=collate_to_test_batch, pin_memory=False, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "a_hSm-KcyCjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "    def forward(self, outputs, tokens, tokens_lens):\n",
        "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1, :, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        return super().forward(packed_outputs, packed_tokens)\n",
        "  \n",
        "class LMAccuracy(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, outputs, tokens, tokens_lens):\n",
        "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1, :, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        return (packed_outputs.argmax(dim=1) == packed_tokens).sum()"
      ],
      "metadata": {
        "id": "oApcakT0yClc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_cipher(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    for idx, data in enumerate(dataloader):\n",
        "        tokens = data[0].to(device)\n",
        "        tokens_lens = data[2].to(device)         \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tokens, tokens_lens)\n",
        "        loss = loss_fn(outputs, tokens, tokens_lens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "def evaluate_cipher(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    accuracy_fn = LMAccuracy()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataloader):\n",
        "            tokens = data[0].to(device)\n",
        "            tokens_lens = data[2].to(device) \n",
        "            outputs = model(tokens, tokens_lens)\n",
        "            num_tokens = (tokens_lens - 1).sum().detach().item()\n",
        "            total_tokens += num_tokens\n",
        "            total_loss += loss_fn(outputs, tokens, tokens_lens) * num_tokens\n",
        "            total_accuracy += accuracy_fn(outputs, tokens, tokens_lens)\n",
        "\n",
        "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
        "\n",
        "def train_cipher(train_loader, model, loss_fn, optimizer, device, num_epochs):\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_cipher(train_loader, model, loss_fn, optimizer, device)\n",
        "        saver_state(model, 'cipher_new_weights' + str(epoch))\n",
        "        train_loss, train_acc = evaluate_cipher(train_loader, model, loss_fn, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        print('Epoch: {0:d}/{1:d}. Loss (Train): {2:.3f}. Accuracy (Train): {3:.3f}'.format(epoch + 1, num_epochs, train_losses[-1], train_accuracies[-1]))\n",
        "    return train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "5AwVZqKbyCn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_curves(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    for idx, data in enumerate(dataloader):\n",
        "        tokens = data[1].to(device)\n",
        "        tokens_lens = data[3].to(device)         \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tokens, tokens_lens)\n",
        "        loss = loss_fn(outputs, tokens, tokens_lens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "def evaluate_curves(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    accuracy_fn = LMAccuracy()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataloader):\n",
        "            tokens = data[1].to(device)\n",
        "            tokens_lens = data[3].to(device) \n",
        "            outputs = model(tokens, tokens_lens)\n",
        "            num_tokens = (tokens_lens - 1).sum().detach().item()\n",
        "            total_tokens += num_tokens\n",
        "            total_loss += loss_fn(outputs, tokens, tokens_lens) * num_tokens\n",
        "            total_accuracy += accuracy_fn(outputs, tokens, tokens_lens)\n",
        "\n",
        "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
        "\n",
        "def train_curves(train_loader, model, loss_fn, optimizer, device, num_epochs):\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_curves(train_loader, model, loss_fn, optimizer, device)\n",
        "        saver_state(model, 'curves_new_weights' + str(epoch))\n",
        "        train_loss, train_acc = evaluate_curves(train_loader, model, loss_fn, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        print('Epoch: {0:d}/{1:d}. Loss (Train): {2:.3f}. Accuracy (Train): {3:.3f}'.format(epoch + 1, num_epochs, train_losses[-1], train_accuracies[-1]))\n",
        "    return train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "-sBuyr97yI6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoderCipher(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, ciphers_vocab, device):\n",
        "        super().__init__()\n",
        "        self.ciphers_embs = torch.nn.Embedding(num_embeddings=len(ciphers_vocab), embedding_dim=embedding_dim, padding_idx=0)\n",
        "        self.lstm = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, len(ciphers_vocab))\n",
        "                \n",
        "    def forward(self, ciphers, ciphers_lens):\n",
        "        ciphers_embeddings = self.ciphers_embs(ciphers)\n",
        "        output, (h, c) = self.lstm(ciphers_embeddings)\n",
        "        output, (h, c) = self.lstm(ciphers_embeddings, (h, c))\n",
        "        return self.linear(output)\n",
        "    \n",
        "class AutoEncoderCurves(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, curves_vocab, device):\n",
        "        super().__init__()\n",
        "        self.curves_embs = torch.nn.Embedding(num_embeddings=len(curves_vocab), embedding_dim=embedding_dim, padding_idx=0)\n",
        "        self.lstm = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, len(curves_vocab))\n",
        "                \n",
        "    def forward(self, curves, curves_lens):\n",
        "        curves_embeddings = self.curves_embs(curves)\n",
        "        output, (h, c) = self.lstm(curves_embeddings)\n",
        "        output, (h, c) = self.lstm(curves_embeddings, (h, c))\n",
        "        return self.linear(output)"
      ],
      "metadata": {
        "id": "LceL_8KlyI8Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_cipher = AutoEncoderCipher(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\n",
        "lm_optimizer = torch.optim.Adam(autoencoder_cipher.parameters(), lr=learning_rate)\n",
        "t = time.time()\n",
        "train_losses, train_accuracies = train_cipher(\n",
        "    train_dataloader, autoencoder_cipher, lm_loss_fn, lm_optimizer, device, 20\n",
        ")\n",
        "print('time:', time.time() - t)"
      ],
      "metadata": {
        "id": "qleVIDXHyI-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_curves = AutoEncoderCurves(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\n",
        "lm_optimizer = torch.optim.Adam(autoencoder_curves.parameters(), lr=learning_rate)\n",
        "t = time.time()\n",
        "train_losses, train_accuracies = train_curves(\n",
        "    train_dataloader, autoencoder_curves, lm_loss_fn, lm_optimizer, device, 20\n",
        ")\n",
        "print('time:', time.time() - t)"
      ],
      "metadata": {
        "id": "ytr23vKuyJAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LMCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        \n",
        "    def forward(self, outputs, tokens, tokens_lens):\n",
        "        tokens_lens = torch.full_like(tokens_lens, outputs.shape[0])\n",
        "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1, :, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        return super().forward(packed_outputs, packed_tokens)\n",
        "\n",
        "class LMAccuracy(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, outputs, tokens, tokens_lens):\n",
        "        tokens_lens = torch.full_like(tokens_lens, outputs.shape[0])\n",
        "        packed_outputs = torch.nn.utils.rnn.pack_padded_sequence(outputs[:-1, :, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        packed_tokens = torch.nn.utils.rnn.pack_padded_sequence(tokens[1:, :], tokens_lens.cpu() - 1, batch_first=False, enforce_sorted=False).data\n",
        "        return (packed_outputs.argmax(dim=1) == packed_tokens).sum()"
      ],
      "metadata": {
        "id": "0AfLbMQJ2_fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_cipher(dataloader, model, loss_fn, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    for idx, data in enumerate(dataloader):\n",
        "        tokens = data[0].to(device)\n",
        "        tokens_lens = data[2].to(device) \n",
        "        tokens_lens = torch.full_like(tokens_lens, tokens.shape[0])\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tokens, tokens_lens)\n",
        "        loss = loss_fn(outputs, tokens, tokens_lens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "def evaluate_cipher(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    accuracy_fn = LMAccuracy()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataloader):\n",
        "            tokens = data[0].to(device)\n",
        "            tokens_lens = data[2].to(device) \n",
        "            tokens_lens = torch.full_like(tokens_lens, tokens.shape[0])\n",
        "            outputs = model(tokens, tokens_lens)\n",
        "            num_tokens = (tokens_lens - 1).sum().detach().item()\n",
        "            total_tokens += num_tokens\n",
        "            total_loss += loss_fn(outputs, tokens, tokens_lens) * num_tokens\n",
        "            total_accuracy += accuracy_fn(outputs, tokens, tokens_lens)\n",
        "\n",
        "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
        "\n",
        "def train_cipher(train_loader, test_loader, model, loss_fn, optimizer, scheduler, device, num_epochs):\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_cipher(train_loader, model, loss_fn, optimizer, scheduler, device)\n",
        "        saver_state(model, 'cipher_back_weights' + str(epoch))\n",
        "        train_loss, train_acc = evaluate_cipher(test_loader, model, loss_fn, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        print('Epoch: {0:d}/{1:d}. Loss (Train): {2:.3f}. Accuracy (Train): {3:.3f}'.format(epoch + 1, num_epochs, train_losses[-1], train_accuracies[-1]))\n",
        "    return train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "FHlymNxD3PDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_curves(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    for idx, data in enumerate(dataloader):\n",
        "        tokens = data[1].to(device)\n",
        "        tokens_lens = data[3].to(device)\n",
        "        tokens_lens = torch.full_like(tokens_lens, tokens.shape[0])\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(tokens, tokens_lens)\n",
        "        loss = loss_fn(outputs, tokens, tokens_lens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "def evaluate_curves(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    accuracy_fn = LMAccuracy()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataloader):\n",
        "            tokens = data[1].to(device)\n",
        "            tokens_lens = data[3].to(device) \n",
        "            tokens_lens = torch.full_like(tokens_lens, tokens.shape[0])\n",
        "            outputs = model(tokens, tokens_lens)\n",
        "            num_tokens = (tokens_lens - 1).sum().detach().item()\n",
        "            total_tokens += num_tokens\n",
        "            total_loss += loss_fn(outputs, tokens, tokens_lens) * num_tokens\n",
        "            total_accuracy += accuracy_fn(outputs, tokens, tokens_lens)\n",
        "\n",
        "    return total_loss / total_tokens, total_accuracy / total_tokens\n",
        "\n",
        "def train_curves(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs):\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_curves(train_loader, model, loss_fn, optimizer, device)\n",
        "        saver_state(model, 'curves_back_weights' + str(epoch))\n",
        "        train_loss, train_acc = evaluate_curves(test_loader, model, loss_fn, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        print('Epoch: {0:d}/{1:d}. Loss (Train): {2:.3f}. Accuracy (Train): {3:.3f}'.format(epoch + 1, num_epochs, train_losses[-1], train_accuracies[-1]))\n",
        "    return train_losses, train_accuracies"
      ],
      "metadata": {
        "id": "nmn2r7Jw3PGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoderCipherBack(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, ciphers_vocab, device):\n",
        "        super().__init__()\n",
        "        self.ciphers_embs = torch.nn.Embedding(num_embeddings=len(ciphers_vocab), embedding_dim=embedding_dim, padding_idx=0)\n",
        "        self.lstm = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, len(ciphers_vocab))\n",
        "                \n",
        "    def forward(self, ciphers, ciphers_lens):\n",
        "        ciphers_embeddings = self.ciphers_embs(ciphers)\n",
        "        ciphers_embeddings = torch.flip(ciphers_embeddings, (0, ))\n",
        "        output, (h, c) = self.lstm(ciphers_embeddings)\n",
        "        output, (h, c) = self.lstm(ciphers_embeddings, (h, c))\n",
        "        return self.linear(output)\n",
        "    \n",
        "class AutoEncoderCurvesBack(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, curves_vocab, device):\n",
        "        super().__init__()\n",
        "        self.curves_embs = torch.nn.Embedding(num_embeddings=len(curves_vocab), embedding_dim=embedding_dim, padding_idx=0)\n",
        "        self.lstm = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.linear = torch.nn.Linear(hidden_dim, len(curves_vocab))\n",
        "                \n",
        "    def forward(self, curves, curves_lens):\n",
        "        curves_embeddings = self.curves_embs(curves)\n",
        "        curves_embeddings = torch.flip(curves_embeddings, (0, ))\n",
        "        output, (h, c) = self.lstm(curves_embeddings)\n",
        "        output, (h, c) = self.lstm(curves_embeddings, (h, c))\n",
        "        return self.linear(output)"
      ],
      "metadata": {
        "id": "hliQQfqK3PI2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_cipher_back = AutoEncoderCipherBack(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "autoencoder_cipher_back.load_state_dict(torch.load('cipher_back_weights5'))\n",
        "lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\n",
        "lm_optimizer = torch.optim.Adam(autoencoder_cipher_back.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(lm_optimizer, gamma = (1e-5/1e-3)**(1/(1243560)))\n",
        "t = time.time()\n",
        "train_losses, train_accuracies = train_cipher(\n",
        "    train_dataloader, train_dataloader, autoencoder_cipher_back, lm_loss_fn, lm_optimizer, scheduler, device, 30\n",
        ")\n",
        "print('time:', time.time() - t)"
      ],
      "metadata": {
        "id": "648NmICJ3qjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_curves_back = AutoEncoderCurvesBack(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "lm_loss_fn = LMCrossEntropyLoss(reduction='mean')\n",
        "lm_optimizer = torch.optim.Adam(autoencoder_curves_back.parameters(), lr=learning_rate)\n",
        "t = time.time()\n",
        "train_losses, train_accuracies = train_curves(\n",
        "    train_dataloader, train_dataloader, autoencoder_curves_back, lm_loss_fn, lm_optimizer, device, 20\n",
        ")\n",
        "print('time:', time.time() - t)"
      ],
      "metadata": {
        "id": "3bjXldNc3qmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Классификатор"
      ],
      "metadata": {
        "id": "jR38Nl65yUZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В классификаторе LSTM проходятся по построенным эмбеддингам шифров и кривых. Также в классификаторе участвуют эмбеддинги user agent. В конце применяется бэггинг 5 аналогичных моделей"
      ],
      "metadata": {
        "id": "_u3vWoqlJ4zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Lowercase()\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"], vocab_size=vocab_size)\n",
        "tokenizer.train_from_iterator(\n",
        "    [f\"{row.ua}\" for row in big_dataset.itertuples()], \n",
        "    trainer=trainer\n",
        ")\n",
        "tokenizer.enable_padding()"
      ],
      "metadata": {
        "id": "dxAjqQPqTkdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDatasetFinal(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.rows = []\n",
        "        for i, row in data.iterrows():\n",
        "            self.rows.append(row)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.rows[idx]\n",
        "\n",
        "        token_ids = tokenizer.encode(row.ua).ids\n",
        "        if (len(token_ids) > 150):\n",
        "            token_ids = token_ids[:150]\n",
        "        ua = torch.tensor(tokenizer.encode(\"[START]\").ids + token_ids + tokenizer.encode(\"[END]\").ids, dtype=torch.long)\n",
        "\n",
        "        ciphers = torch.tensor(ciphers_vocab.lookup_indices(['<sos>']) + ciphers_vocab.lookup_indices(list(row.ciphers)) + ciphers_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        curves = torch.tensor(curves_vocab.lookup_indices(['<sos>']) + curves_vocab.lookup_indices(list(row.curves)) + curves_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        return ua, ciphers, curves, torch.tensor(len(token_ids) + 2), torch.tensor(len(row.ciphers) + 2), torch.tensor(len(row.curves) + 2), torch.tensor(row.label, dtype=torch.float)\n",
        "    \n",
        "class TestDatasetFinal(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.data.reset_index(drop=True, inplace=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.rows = []\n",
        "        for i, row in data.iterrows():\n",
        "            self.rows.append(row)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.rows[idx]\n",
        "\n",
        "        token_ids = tokenizer.encode(row.ua).ids\n",
        "        if (len(token_ids) > 150):\n",
        "            token_ids = token_ids[:150]\n",
        "        ua = torch.tensor(tokenizer.encode(\"[START]\").ids + token_ids + tokenizer.encode(\"[END]\").ids, dtype=torch.long)\n",
        "\n",
        "        ciphers = torch.tensor(ciphers_vocab.lookup_indices(['<sos>']) + ciphers_vocab.lookup_indices(list(row.ciphers)) + ciphers_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        curves = torch.tensor(curves_vocab.lookup_indices(['<sos>']) + curves_vocab.lookup_indices(list(row.curves)) + curves_vocab.lookup_indices(['<eos>']), dtype=torch.long)\n",
        "        return ua, ciphers, curves, torch.tensor(len(token_ids) + 2), torch.tensor(len(row.ciphers) + 2), torch.tensor(len(row.curves) + 2)\n",
        "\n",
        "def collate_to_train_batch_final(batch):\n",
        "    ua = torch.nn.utils.rnn.pad_sequence([i[0] for i in batch], padding_value=0)\n",
        "    ciphers = torch.nn.utils.rnn.pad_sequence([i[1] for i in batch], padding_value=0)\n",
        "    curves = torch.nn.utils.rnn.pad_sequence([i[2] for i in batch], padding_value=0)\n",
        "    return ua, ciphers, curves, torch.tensor([i[3] for i in batch], dtype=torch.long), torch.tensor([i[4] for i in batch], dtype=torch.long), torch.tensor([i[5] for i in batch], dtype=torch.long), torch.tensor([i[6] for i in batch], dtype=torch.float)\n",
        "\n",
        "def collate_to_test_batch_final(batch):\n",
        "    ua = torch.nn.utils.rnn.pad_sequence([i[0] for i in batch], padding_value=0)\n",
        "    ciphers = torch.nn.utils.rnn.pad_sequence([i[1] for i in batch], padding_value=0)\n",
        "    curves = torch.nn.utils.rnn.pad_sequence([i[2] for i in batch], padding_value=0)\n",
        "    return ua, ciphers, curves, torch.tensor([i[3] for i in batch], dtype=torch.long), torch.tensor([i[4] for i in batch], dtype=torch.long), torch.tensor([i[5] for i in batch], dtype=torch.long)"
      ],
      "metadata": {
        "id": "UmlsdRd7yCqN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_final(dataloader, model, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    for idx, data in enumerate(dataloader):\n",
        "        ua = data[0].to(device)\n",
        "        ciphers = data[1].to(device)\n",
        "        curves = data[2].to(device)\n",
        "        ua_lens = data[3].to(device)\n",
        "        ciphers_lens = data[4].to(device)\n",
        "        curves_lens = data[5].to(device)\n",
        "        labels = data[6].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(ua, ciphers, curves, ua_lens, ciphers_lens, curves_lens)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "def evaluate_final(dataloader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(dataloader):\n",
        "            ua = data[0].to(device)\n",
        "            ciphers = data[1].to(device)\n",
        "            curves = data[2].to(device)\n",
        "            ua_lens = data[3].to(device)\n",
        "            ciphers_lens = data[4].to(device)\n",
        "            curves_lens = data[5].to(device)\n",
        "            labels = data[6].to(device)\n",
        "            outputs = model(ua, ciphers, curves, ua_lens, ciphers_lens, curves_lens)\n",
        "            total_loss += loss_fn(outputs, labels) * len(outputs)\n",
        "            if not idx:\n",
        "                y_pred = torch.sigmoid(outputs).cpu().numpy()\n",
        "                y_true = labels.cpu().numpy()\n",
        "            else:\n",
        "                y_pred = np.concatenate((y_pred, torch.sigmoid(outputs).cpu().numpy()))\n",
        "                y_true = np.concatenate((y_true, labels.cpu().numpy()))\n",
        "        \n",
        "    return total_loss / len(dataloader.dataset), roc_auc_score(y_true, y_pred)\n",
        "\n",
        "def train_final(train_loader, test_loader, model, loss_fn, optimizer, scheduler, device, num_epochs):\n",
        "    test_losses = []\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    train_accuracies = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_final(train_loader, model, loss_fn, optimizer, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss, train_acc = evaluate_final(train_loader, model, loss_fn, device)\n",
        "        train_accuracies.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        test_loss, test_acc = evaluate_final(test_loader, model, loss_fn, device)\n",
        "        test_accuracies.append(test_acc)\n",
        "        test_losses.append(test_loss)\n",
        "        \n",
        "        print(\n",
        "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. ROC-AUC (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
        "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
        "            )\n",
        "        )\n",
        "    return train_losses, train_accuracies, test_losses, test_accuracies\n",
        "\n",
        "def train_submission(train_loader, test_dataloader, model, loss_fn, optimizer, scheduler, device, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch_final(train_loader, model, loss_fn, optimizer, device)\n",
        "        train_loss, train_acc = evaluate_final(train_loader, model, loss_fn, device)\n",
        "        scheduler.step()\n",
        "        print('Epoch: {0:d}/{1:d}. Loss (Train): {2:.3f}. ROC-AUC (Train): {3:.3f}'.format(epoch + 1, num_epochs, train_loss, train_acc))\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(test_dataloader):\n",
        "            ua = data[0].to(device)\n",
        "            ciphers = data[1].to(device)\n",
        "            curves = data[2].to(device)\n",
        "            ua_lens = data[3].to(device)\n",
        "            ciphers_lens = data[4].to(device)\n",
        "            curves_lens = data[5].to(device)\n",
        "            if not idx:\n",
        "                outputs = model(ua, ciphers, curves, ua_lens, ciphers_lens, curves_lens)\n",
        "            else:\n",
        "                outputs = torch.cat((outputs, model(ua, ciphers, curves, ua_lens, ciphers_lens, curves_lens)), dim=0)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "RB3LNY5WyCsP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_cipher = AutoEncoderCipher(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "autoencoder_cipher.load_state_dict(torch.load('cipher_new_weights16'))\n",
        "autoencoder_curves = AutoEncoderCurves(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "autoencoder_curves.load_state_dict(torch.load('curves_new_weights19'))\n",
        "autoencoder_cipher_back = AutoEncoderCipherBack(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "autoencoder_cipher_back.load_state_dict(torch.load('cipher_back_weights5'))\n",
        "autoencoder_curves_back = AutoEncoderCurvesBack(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "autoencoder_curves_back.load_state_dict(torch.load('curves_back_weights15'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md0Ls3Nm5gmJ",
        "outputId": "3d9413db-b952-4c3f-c536-485f1a32c02a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNFinal(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, embedding_dim_ua, hidden_dim_ua, ciphers_vocab, curves_vocab, autoencoder_cipher, autoencoder_curves, autoencoder_cipher_back, autoencoder_curves_back):\n",
        "        super().__init__()\n",
        "        self.lstm1 = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.lstm2 = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.lstm3 = FastRNNLayer(embedding_dim_ua, hidden_dim_ua, 0.25)\n",
        "        self.lstm4 = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.lstm5 = FastRNNLayer(embedding_dim, hidden_dim, 0.25)\n",
        "        self.lstm6 = FastRNNLayer(embedding_dim_ua, hidden_dim_ua, 0.25)\n",
        "        self.linear1 = torch.nn.Linear(4 * hidden_dim + 2 * hidden_dim_ua, hidden_dim)\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = torch.nn.Linear(hidden_dim, 1)\n",
        "        self.relu1 = torch.nn.ReLU()\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.autoencoder_cipher = autoencoder_cipher\n",
        "        self.autoencoder_curves = autoencoder_curves\n",
        "        self.autoencoder_cipher_back = autoencoder_cipher_back\n",
        "        self.autoencoder_curves_back = autoencoder_curves_back\n",
        "        self.ua_embs = torch.nn.Embedding(num_embeddings=1000, embedding_dim=embedding_dim_ua, padding_idx=0)\n",
        "        self.ua_embs_back = torch.nn.Embedding(num_embeddings=1000, embedding_dim=embedding_dim_ua, padding_idx=0)\n",
        "                \n",
        "    def forward(self, ua, ciphers, curves, ua_lens, ciphers_lens, curves_lens):\n",
        "        ciphers_embeddings = self.autoencoder_cipher.ciphers_embs(ciphers)\n",
        "        curves_embeddings = self.autoencoder_curves.curves_embs(curves)\n",
        "        ciphers_embeddings_back = self.autoencoder_cipher_back.ciphers_embs(ciphers)\n",
        "        curves_embeddings_back = self.autoencoder_curves_back.curves_embs(curves)\n",
        "        ua_embeddings = self.ua_embs(ua)\n",
        "        ua_embeddings_back = self.ua_embs_back(ua)\n",
        "        output1 = self.lstm1(ciphers_embeddings)[0]\n",
        "        output2 = self.lstm2(curves_embeddings)[0]\n",
        "        output3 = self.lstm3(ua_embeddings)[0]\n",
        "        output4, (h1, _) = self.lstm4(torch.flip(ciphers_embeddings_back, (0, )))\n",
        "        output5, (h2, _) = self.lstm5(torch.flip(curves_embeddings_back, (0, )))\n",
        "        output6, (h3, _) = self.lstm6(torch.flip(ua_embeddings_back, (0, )))\n",
        "        output = torch.cat((output1[ciphers_lens - 1, torch.arange(output1.shape[1]), :], output2[curves_lens - 1, torch.arange(output2.shape[1]), :], output3[ua_lens - 1, torch.arange(output3.shape[1]), :], torch.squeeze(h1), torch.squeeze(h2), torch.squeeze(h3)), 1)\n",
        "        output = self.linear3(self.relu2(self.linear2(self.relu1(self.linear1(output)))))\n",
        "        output = torch.squeeze(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "k9psPrBzHy0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(TestDatasetFinal(test_dataset, tokenizer), batch_size=batch_size, collate_fn=collate_to_test_batch_final, pin_memory=False, shuffle=False)\n",
        "for i, (train_index, test_index) in enumerate(skf.split(train_dataset.curves.values, train_dataset.label.values)):\n",
        "    train_dataloader = torch.utils.data.DataLoader(TrainDatasetFinal(train_dataset.loc[train_index], tokenizer), batch_size=batch_size, collate_fn=collate_to_train_batch_final, pin_memory=False, shuffle=True)\n",
        "    autoencoder_cipher = AutoEncoderCipher(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "    autoencoder_cipher.load_state_dict(torch.load('cipher_new_weights16'))\n",
        "    autoencoder_curves = AutoEncoderCurves(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "    autoencoder_curves.load_state_dict(torch.load('curves_new_weights19'))\n",
        "    autoencoder_cipher_back = AutoEncoderCipherBack(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "    autoencoder_cipher_back.load_state_dict(torch.load('cipher_back_weights5'))\n",
        "    autoencoder_cipher_back = AutoEncoderCipher(embedding_dim, hidden_dim, ciphers_vocab, device).to(device=device)\n",
        "    autoencoder_cipher_back.load_state_dict(torch.load('cipher_new_weights16'))\n",
        "    autoencoder_curves_back = AutoEncoderCurvesBack(embedding_dim, hidden_dim, curves_vocab, device).to(device=device)\n",
        "    autoencoder_curves_back.load_state_dict(torch.load('curves_back_weights15'))\n",
        "    model = RNNFinal(embedding_dim, hidden_dim, embedding_dim_ua, hidden_dim_ua, ciphers_vocab, curves_vocab, autoencoder_cipher, autoencoder_curves, autoencoder_cipher_back, autoencoder_curves_back).to(device)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = (5e-5/1e-3)**(1/(num_epochs)))\n",
        "    submission = train_submission(train_dataloader, test_dataloader, model, loss_fn, optimizer, scheduler, device, num_epochs)\n",
        "    submission = torch.sigmoid(submission).detach().cpu().numpy()\n",
        "    if i == 0:\n",
        "        total_submission = submission\n",
        "    else:\n",
        "        total_submission += submission"
      ],
      "metadata": {
        "id": "BpjafMWMH2ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_submission = pd.read_csv('sample_submission.csv')\n",
        "submission = total_submission / 5\n",
        "my_submission.is_bot = submission\n",
        "my_submission.to_csv(\"submission_final.csv\", index=None)"
      ],
      "metadata": {
        "id": "1Ju7_SfnH4yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATDabU22MPBY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}